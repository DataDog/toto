{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory to the Toto module\n",
    "%cd ../../toto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "LIGHT_BENCHMARK = False\n",
    "\n",
    "NON_ZERO_METRICS = [\n",
    "    \"eval_metrics/MASE[0.5]\",\n",
    "    \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "]\n",
    "ZERO_METRICS = [\n",
    "    \"eval_metrics/MAE[0.5]\",\n",
    "    \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "]\n",
    "LOW_VARIANCE_DATASETS = [\n",
    "    \"ds-1040-5T\",\n",
    "    \"ds-462-5T\",\n",
    "    \"ds-2801-D\",\n",
    "    \"ds-947-T\",\n",
    "    \"ds-2332-H\",\n",
    "    \"ds-491-T\",\n",
    "    \"ds-1901-D\",\n",
    "    \"ds-1721-30T\",\n",
    "    \"ds-2806-D\",\n",
    "    \"ds-438-5T\",\n",
    "    \"ds-372-10S\",\n",
    "    \"ds-1719-30T\",\n",
    "    \"ds-1902-D\",\n",
    "    \"ds-111-T\",\n",
    "    \"ds-2026-D\",\n",
    "    \"ds-2089-H\",\n",
    "    \"ds-299-5T\",\n",
    "    \"ds-1596-T\",\n",
    "    \"ds-4-5T\",\n",
    "    \"ds-1723-H\",\n",
    "    \"ds-953-10S\",\n",
    "    \"ds-2394-D\",\n",
    "    \"ds-1838-D\",\n",
    "    \"ds-489-T\",\n",
    "    \"ds-2802-D\",\n",
    "    \"ds-1925-D\",\n",
    "    \"ds-441-10S\",\n",
    "    \"ds-1520-10S\",\n",
    "    \"ds-2782-H\",\n",
    "    \"ds-607-10S\",\n",
    "    \"ds-177-5T\",\n",
    "    \"ds-1643-30T\",\n",
    "    \"ds-442-T\",\n",
    "    \"ds-1909-D\",\n",
    "    \"ds-2762-D\",\n",
    "    \"ds-1135-5T\",\n",
    "    \"ds-139-5T\",\n",
    "    \"ds-805-10S\",\n",
    "    \"ds-784-T\",\n",
    "    \"ds-949-T\",\n",
    "    \"ds-977-5T\",\n",
    "    \"ds-1641-30T\",\n",
    "    \"ds-303-5T\",\n",
    "    \"ds-162-5T\",\n",
    "    \"ds-608-T\",\n",
    "    \"ds-551-T\",\n",
    "    \"ds-2567-30T\",\n",
    "    \"ds-1731-D\",\n",
    "    \"ds-2206-H\",\n",
    "    \"ds-206-5T\",\n",
    "    \"ds-1718-H\",\n",
    "    \"ds-1722-D\",\n",
    "    \"ds-1195-10S\",\n",
    "    \"ds-2514-H\",\n",
    "    \"ds-1264-5T\",\n",
    "    \"ds-1720-H\",\n",
    "    \"ds-1564-10S\",\n",
    "    \"ds-532-10S\",\n",
    "    \"ds-300-5T\",\n",
    "    \"ds-207-5T\",\n",
    "    \"ds-1039-T\",\n",
    "    \"ds-1733-D\",\n",
    "    \"ds-2805-D\",\n",
    "    \"ds-979-T\",\n",
    "    \"ds-1619-H\",\n",
    "    \"ds-1818-H\",\n",
    "    \"ds-492-T\",\n",
    "    \"ds-1894-H\",\n",
    "    \"ds-2804-D\",\n",
    "    \"ds-2012-30T\",\n",
    "    \"ds-181-T\",\n",
    "    \"ds-1642-30T\",\n",
    "    \"ds-2027-H\",\n",
    "    \"ds-890-10S\",\n",
    "    \"ds-1768-30T\",\n",
    "    \"ds-458-10S\",\n",
    "    \"ds-1767-H\",\n",
    "    \"ds-137-10S\",\n",
    "    \"ds-2197-D\",\n",
    "    \"ds-493-T\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shifted_gmean(x, epsilon=1e-5, dim=-1):\n",
    "    logsum = np.sum(np.log(x + epsilon))\n",
    "    n = x.shape[dim]\n",
    "    return np.exp(logsum / n) - epsilon\n",
    "\n",
    "def load_and_process_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"full_dataset_name\"] = df[\"dataset\"]\n",
    "    df[\"dataset\"] = df[\"dataset\"].str.split(\"/\").str[0]\n",
    "    if LIGHT_BENCHMARK:\n",
    "        light_benchmark_datasets = json.load(open(\"../boom/dataset_properties_light.json\")).keys()\n",
    "        df = df[df[\"dataset\"].isin(light_benchmark_datasets)]\n",
    "    return df\n",
    "\n",
    "def load_model_results(models_dir):\n",
    "    model_names = [d for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))]\n",
    "    dfs = [load_and_process_csv(os.path.join(models_dir, m, \"all_results.csv\")) for m in model_names]\n",
    "\n",
    "    assert \"seasonalnaive\" in model_names, \"seasonalnaive model must be present in models directory\"\n",
    "    \n",
    "    i = model_names.index(\"seasonalnaive\")\n",
    "    model_names.append(model_names.pop(i))\n",
    "    dfs.append(dfs.pop(i))\n",
    "    \n",
    "    print(f'Number of models in leaderboard: {len(dfs)} \\n')\n",
    "    return dfs, model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def separate_zero_inflated_data(dfs):\n",
    "    zero_datasets = dfs[-1][(dfs[-1][\"eval_metrics/MASE[0.5]\"] == 0)][\"dataset\"].unique()\n",
    "    datasets_to_exclude = set(LOW_VARIANCE_DATASETS) | set(zero_datasets)\n",
    "    print(f\"Number of datasets to exclude: {len(datasets_to_exclude)}\\n\")\n",
    "    \n",
    "    non_zero_dfs = [df[~df[\"dataset\"].isin(datasets_to_exclude)] for df in dfs]\n",
    "    zero_dfs = [df[df[\"dataset\"].isin(datasets_to_exclude)] for df in dfs]\n",
    "\n",
    "    return non_zero_dfs, zero_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_by_naive(df, naive_df, metrics):\n",
    "    assert set(df[\"full_dataset_name\"]) == set(naive_df[\"full_dataset_name\"]), \"All datasets must be the same\"\n",
    "\n",
    "    merged = df.merge(naive_df, on=\"full_dataset_name\", suffixes=(\"\", \"_naive\"))\n",
    "    for col in metrics:\n",
    "        merged[col] = merged[col] / merged[f\"{col}_naive\"]\n",
    "    return merged[df.columns]\n",
    "\n",
    "\n",
    "def replace_invalid_values(dfs, metrics):\n",
    "    cleaned_dfs = []\n",
    "\n",
    "    for df in dfs:\n",
    "        df = df.copy()\n",
    "        df[metrics] = df[metrics].replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "        column_means = df[metrics].mean()\n",
    "        df[metrics] = df[metrics].fillna(column_means)\n",
    "\n",
    "        cleaned_dfs.append(df[metrics + [\"full_dataset_name\", \"dataset\"]])\n",
    "\n",
    "    return cleaned_dfs\n",
    "\n",
    "def process_dd_benchmark_model_results(is_scale_by_naive, dfs, metrics): \n",
    "    dfs = replace_invalid_values(dfs, metrics)\n",
    "    if is_scale_by_naive:\n",
    "        dfs = [scale_by_naive(df, dfs[-1], metrics) for df in dfs]\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_number(num):\n",
    "    # Check if the value is numeric\n",
    "    if isinstance(num, (int, float)):\n",
    "        if abs(num) >= 10**2:\n",
    "            return f\"{num:.1e}\"\n",
    "        else:\n",
    "            return f\"{num:.3f}\"\n",
    "    # Return non-numeric values as-is\n",
    "    return num\n",
    "\n",
    "def rename_metrics(df):\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"eval_metrics/MASE[0.5]\": \"MASE\",\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\": \"CRPS\",\n",
    "            \"rank\": \"Rank\",\n",
    "        }\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def get_leaderboard(dfs, names, agg_func, metrics, ranking_metric=\"eval_metrics/mean_weighted_sum_quantile_loss\"):\n",
    "\n",
    "    for df, name in zip(dfs, names):\n",
    "        df[\"model\"] = name\n",
    "\n",
    "    combined_df = pd.concat(dfs)\n",
    "    combined_df[\"rank\"] = combined_df.groupby(\"full_dataset_name\")[ranking_metric].rank(method=\"first\", ascending=True)\n",
    "    aggregation_functions = {metric: agg_func for metric in metrics}\n",
    "    aggregation_functions[\"rank\"] = \"mean\"\n",
    "    agg = combined_df[[\"model\"] + metrics + [\"rank\"]].groupby(\"model\").agg(aggregation_functions).reset_index()\n",
    "\n",
    "    # Create and format the leaderboard\n",
    "    leaderboard = agg.set_index(\"model\").sort_values(by=\"rank\", ascending=True).map(format_number)\n",
    "\n",
    "    return rename_metrics(leaderboard)\n",
    "\n",
    "\n",
    "def get_separate_zero_inflated_leaderboard(non_zero_dfs, zero_dfs, dfs_names, agg_func, non_zero_metrics, zero_metrics):\n",
    "\n",
    "    non_zero_leaderboard = get_leaderboard(non_zero_dfs, dfs_names, agg_func, metrics=non_zero_metrics)\n",
    "    zero_leaderboard = get_leaderboard(zero_dfs, dfs_names, agg_func, metrics=zero_metrics)\n",
    "\n",
    "    non_zero_count = len(non_zero_dfs[0])\n",
    "    zero_count = len(zero_dfs[0])\n",
    "\n",
    "    non_zero_leaderboard.columns = [f\"{col}-{non_zero_count}-scaled\" for col in non_zero_leaderboard.columns]\n",
    "    zero_leaderboard.columns = [f\"{col}-{zero_count}-unscaled\" for col in zero_leaderboard.columns]\n",
    "\n",
    "    combined_leaderboard = pd.merge(non_zero_leaderboard, zero_leaderboard, on=\"model\", suffixes=(\"_non_zero\", \"_zero\"))\n",
    "    return combined_leaderboard\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs, dfs_names = load_model_results('../results/')\n",
    "non_zero_dfs, zero_dfs = separate_zero_inflated_data(dfs)\n",
    "\n",
    "non_zero_dfs = process_dd_benchmark_model_results(\n",
    "    is_scale_by_naive=True,\n",
    "    dfs=non_zero_dfs,\n",
    "    metrics=NON_ZERO_METRICS,\n",
    ")\n",
    "\n",
    "zero_dfs = process_dd_benchmark_model_results(\n",
    "    is_scale_by_naive=False, \n",
    "    dfs=zero_dfs,\n",
    "    metrics=ZERO_METRICS,\n",
    ")\n",
    "\n",
    "os.makedirs('../leaderboards/', exist_ok=True)\n",
    "leaderboard = get_separate_zero_inflated_leaderboard(\n",
    "        non_zero_dfs=non_zero_dfs,\n",
    "        zero_dfs=zero_dfs,\n",
    "        dfs_names=dfs_names,\n",
    "        agg_func=shifted_gmean,\n",
    "        non_zero_metrics=NON_ZERO_METRICS,\n",
    "        zero_metrics=ZERO_METRICS,\n",
    "    )\n",
    "\n",
    "leaderboard.to_csv(f'../leaderboards/{\"light\" if LIGHT_BENCHMARK else \"full\"}_leaderboard.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
